# Policy Gradient (REINFORCE) Implementation

This directory contains a custom implementation of Policy Gradient (REINFORCE) algorithm for Atari games.

> **ğŸ“š For general Atari information, see [atari/README.md](../README.md)**

## ğŸ“ Structure

```
pg/
â”œâ”€â”€ scripts/               # Training scripts
â”‚   â”œâ”€â”€ pgpong.py         # Train PG on Pong
â”‚   â”œâ”€â”€ pgbreakout.py     # Train PG on Breakout
â”‚   â”œâ”€â”€ pgpacman.py       # Train PG on Ms. Pacman
â”‚   â”œâ”€â”€ pg_trainer.py     # Generic PG trainer
â”‚   â””â”€â”€ pg_tester.py      # Test trained PG models
â””â”€â”€ src/                   # Source code
    â”œâ”€â”€ agent.py          # PG agent implementation
    â”œâ”€â”€ game.py           # Game environment wrapper
    â”œâ”€â”€ memory.py         # Episode memory buffer
    â”œâ”€â”€ mlp_torch.py      # PyTorch MLP implementation
    â”œâ”€â”€ mlp.py            # NumPy MLP (legacy)
    â”œâ”€â”€ hyperparameters.py # Training hyperparameters
    â””â”€â”€ pacman/           # Pacman-specific implementations
        â”œâ”€â”€ multi_action_agent.py
        â”œâ”€â”€ cnn_torch_multiaction.py
        â”œâ”€â”€ mlp_torch_multiaction.py
        â””â”€â”€ preprocess_pacman.py
```

## ğŸš€ Quick Start

```bash
# Train on Pong (binary actions)
python scripts/pgpong.py

# Train on Breakout (binary actions)
python scripts/pgbreakout.py

# Train on Ms. Pacman (9 actions)
python scripts/pgpacman.py

# Use generic trainer
python scripts/pg_trainer.py pong --render

# Test trained model
python scripts/pg_tester.py --model ../../models/pg/pong/torch_mlp_pong_i6400_h200_o1_90000
```

## ğŸ§  Policy Gradient Features

- **REINFORCE Algorithm**: Direct policy optimization
- **PyTorch Implementation**: Modern MLP with GPU support
- **CNN Support**: Convolutional networks for Pacman
- **Episode Memory**: Stores complete episodes for training
- **Multi-Action Support**: 9-action space for Pacman
- **Frame Preprocessing**: Optimized image processing

## ğŸ—ï¸ Architecture

### MLP Architecture (Pong/Breakout)
- **Input**: 6400 units (80x80 preprocessed frames)
- **Hidden**: 200 units with ReLU activation
- **Output**: 1 unit with Sigmoid activation
- **Policy**: Binary action selection (UP/DOWN or LEFT/RIGHT)

### CNN Architecture (Pacman)
- **Input**: 7 channels at 80x80 resolution
- **Conv1**: 32 filters, 8x8 kernel, stride 4
- **Conv2**: 64 filters, 4x4 kernel, stride 2
- **Conv3**: 64 filters, 3x3 kernel, stride 1
- **Fully Connected**: 200 hidden units
- **Output**: 9 action probabilities

## âš™ï¸ Configuration

Key hyperparameters in `src/hyperparameters.py`:
```python
# Training
learning_rate = 1e-3
batch_size = 10
max_episodes = 100000

# Network
input_size = 6400
hidden_size = 200
output_size = 1  # or 9 for Pacman

# Exploration
temperature = 1.0
```

## ğŸ“Š Performance

- **Pong**: 20+ average reward after 70k episodes
- **Breakout**: Good paddle control and ball tracking
- **Pacman**: Complex maze navigation with ghost avoidance
- **Training Time**: 2-4 hours for 100k episodes on CPU

## ğŸ’¾ Model Storage

Trained models are saved in:
- `../../models/pg/pong/` for Pong models
- `../../models/pg/breakout/` for Breakout models
- `../../models/pg/pacman/` for Pacman models

### Model Naming Convention
```
{base_name}_{game_name}_i{input_size}_h{hidden_size}_o{output_size}_{episode}
```

## ğŸ”§ Dependencies

```bash
pip install torch numpy gymnasium[atari] ale-py
```

## ğŸ“ˆ Training Process

1. **Episode Collection**: Run complete episodes with current policy
2. **Reward Calculation**: Compute discounted rewards for each step
3. **Policy Update**: Update network weights using REINFORCE
4. **Exploration**: Use temperature scheduling for exploration
5. **Model Saving**: Save checkpoints every N episodes

## ğŸ® Game-Specific Implementations

### Pong & Breakout
- **Action Space**: Binary (UP/DOWN for Pong, LEFT/RIGHT for Breakout)
- **Network**: MLP with 6400 input units, 200 hidden units
- **Policy**: Sigmoid output for binary action selection

### Ms. Pacman
- **Action Space**: 9 discrete actions (8 directions + NOOP)
- **Network**: CNN with 7 input channels (color masks + grayscale)
- **Policy**: Softmax output for multi-action selection
- **Preprocessing**: Color-aware preprocessing for ghost detection

## ğŸ” Key Differences from DQN

| Aspect | Policy Gradient | DQN |
|--------|----------------|-----|
| **Learning Type** | On-policy | Off-policy |
| **Action Space** | Both | Discrete only |
| **Policy Type** | Stochastic | Deterministic |
| **Sample Efficiency** | Lower | Higher |
| **Training Stability** | Less stable | More stable |
| **Implementation** | Simpler | More complex |
| **Best For** | Continuous actions | Discrete actions |

## ğŸ¯ REINFORCE Algorithm

The REINFORCE algorithm directly optimizes the policy:

1. **Collect Episode**: Run policy Ï€(s) for complete episode
2. **Compute Returns**: Calculate discounted rewards R(t)
3. **Update Policy**: âˆ‡Î¸ J(Î¸) = E[âˆ‡Î¸ log Ï€(a|s) R(t)]
4. **Repeat**: Continue until convergence

## ğŸ”— Related Documentation

- **[atari/README.md](../README.md)** - General Atari information
- **[dqn/README.md](../dqn/README.md)** - DQN comparison
- **[baselines/README.md](../baselines/README.md)** - Stable Baselines3 A2C 